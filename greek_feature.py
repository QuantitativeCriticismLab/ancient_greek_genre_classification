# -*- coding: utf-8 -*-

import os
import sys
import math
from cltk.tokenize.sentence import TokenizeSentence
from cltk.tokenize.word import WordTokenizer
from cltk.stem.lemma import LemmaReplacer
from unicodedata import normalize #the cltk_normalize cannot decompose (only has NFC & NFKC, not NFD or NFKD)
#Reference: https://jktauber.com/articles/python-unicode-ancient-greek/

class Features:
	def freq_interrogatives(file):
		file = TokenizeSentence("greek").tokenize_sentences(file)
		num_interrogative = 0

		for line in file:
			num_interrogative += line.count(';') + line.count('Í¾')

		return num_interrogative / len(file)

	def freq_conditional_characters(file):
		file = WordTokenizer('greek').tokenize(file)
		num_conditional_characters = 0
		num_characters = 0
		conditional_characters = {'Îµá¼°', 'Îµá¼´', 'Îµá¼²', 'á¼Î¬Î½', 'á¼á½°Î½'}
		conditional_characters = conditional_characters | \
		{normalize('NFD', val) for val in conditional_characters} | \
		{normalize('NFC', val) for val in conditional_characters} | \
		{normalize('NFKD', val) for val in conditional_characters} | \
		{normalize('NFKC', val) for val in conditional_characters}

		for word in file:
			num_conditional_characters += len(word) if word in conditional_characters else 0
			num_characters += len(word)

		return num_conditional_characters / num_characters

	def freq_personal_pronouns(file):
		file = WordTokenizer('greek').tokenize(file)
		num_pronouns = 0
		num_characters = 0
		personal_pronouns = {'á¼Î³ÏŽ', 'á¼Î³á½¼', 'á¼Î¼Î¿á¿¦', 'Î¼Î¿Ï…', 'á¼Î¼Î¿Î¯', 'á¼Î¼Î¿á½¶', 'Î¼Î¿Î¹', 'á¼Î¼Î­', 'á¼Î¼á½²', 'Î¼Îµ', 'á¼¡Î¼Îµá¿–Ï‚', 'á¼¡Î¼á¿¶Î½', \
		'á¼¡Î¼á¿–Î½', 'á¼¡Î¼á¾¶Ï‚', 'ÏƒÏ', 'Ïƒá½º', 'ÏƒÎ¿á¿¦', 'ÏƒÎ¿Ï…', 'ÏƒÎ¿Î¯', 'ÏƒÎ¿á½¶', 'ÏƒÎ¿Î¹', 'ÏƒÎ­', 'Ïƒá½²', 'ÏƒÎµ', 'á½‘Î¼Îµá¿–Ï‚', 'á½‘Î¼á¿¶Î½', 'á½‘Î¼á¿–Î½', 'á½‘Î¼á¾¶Ï‚'}
		personal_pronouns = personal_pronouns | \
		{normalize('NFD', val) for val in personal_pronouns} | \
		{normalize('NFC', val) for val in personal_pronouns} | \
		{normalize('NFKD', val) for val in personal_pronouns} | \
		{normalize('NFKC', val) for val in personal_pronouns}

		for word in file:
			num_pronouns += len(word) if word in personal_pronouns else 0
			num_characters += len(word)

		return num_pronouns / num_characters

	def freq_demonstrative(file):
		file = LemmaReplacer('greek').lemmatize(file)
		num_demonstratives = 0
		num_characters = 0

		for word in file:
			num_demonstratives += len(word) if word == 'á¼ÎºÎµÎ¹Í‚Î½Î¿Ï‚' or word == 'Î±á½•Ï„Î±Î¹' or word == 'Î¿á½—Ï„Î¿Ï‚' or word == 'Ï„ÏŒÎ´Îµ' \
			or word == 'Ï„Î¬Î´Îµ' or word == 'á¼ÎºÎµÎ¯Î½á¾±Ï‚' or word == 'Ï„Î¬ÏƒÏƒÏ‰' or word == 'Ï„Î±ÏÏ„á¾±Ï‚' or word == 'á½…Î´Îµ' else 0
			num_characters += len(word)

		return num_demonstratives / num_characters

	def freq_indefinite_pronoun_in_non_interrogative_sentence(file):
		file = TokenizeSentence("greek").tokenize_sentences(file)
		num_indefinite_pronoun_chars = 0
		num_characters = 0
		interrogative_chars = {';', 'Í¾'}
		pronoun_chars = {'Ï„Î¹Ï‚', 'Ï„Î¹Î½ÏŒÏ‚', 'Ï„Î¹Î½á½¸Ï‚', 'Ï„Î¿Ï…', 'Ï„Î¹Î½Î¯', 'Ï„Î¹Î½á½¶', 'Ï„á¿³', 'Ï„Î¹Î½Î¬', 'Ï„Î¹Î½á½°', 'Ï„Î¹Î½Î­Ï‚', 'Ï„Î¹Î½á½²Ï‚', 'Ï„Î¹Î½á¿¶Î½', \
		'Ï„Î¹ÏƒÎ¯', 'Ï„Î¹Ïƒá½¶', 'Ï„Î¹ÏƒÎ¯Î½', 'Ï„Î¹Ïƒá½¶Î½', 'Ï„Î¹Î½Î¬Ï‚', 'Ï„Î¹Î½á½°Ï‚', 'Ï„Î¹'}
		pronoun_chars = pronoun_chars | \
		{normalize('NFD', val) for val in pronoun_chars} | \
		{normalize('NFC', val) for val in pronoun_chars} | \
		{normalize('NFKD', val) for val in pronoun_chars} | \
		{normalize('NFKC', val) for val in pronoun_chars}

		for line in file:
			if line[-1] not in interrogative_chars:
				line = WordTokenizer('greek').tokenize(line)
				for word in line:
					num_indefinite_pronoun_chars += len(word) if word in pronoun_chars else 0
					num_characters += len(word)

		return num_indefinite_pronoun_chars / num_characters

	def freq_allos(file):
		file = LemmaReplacer('greek').lemmatize(file)
		num_allos = 0
		num_characters = 0

		for word in file:
			num_allos += len(word) if word == 'á¼„Î»Î»Î¿Ï‚' or word == 'á¼„Î»Î»á¾±Ï‚' else 0
			num_characters += len(word)

		return num_allos / num_characters

	def freq_autos(file):
		file = LemmaReplacer('greek').lemmatize(file)
		num_autos = 0
		num_characters = 0

		for word in file:
			num_autos += len(word) if word == 'Î±Ï…Ì“Ï„ÏŒÏ‚' or word == 'Î±á½Ï„á¾±Ï‚' else 0
			num_characters += len(word)

		return num_autos / num_characters

	def freq_reflexive(file):
		file = LemmaReplacer('greek').lemmatize(file)
		num_reflexive = 0
		num_characters = 0

		for word in file:
			num_reflexive += len(word) if word == 'á¼Î¼Î±Ï…Ï„Î¿Ï…Í‚' or word == 'ÏƒÎ±Ï…Ï„Î¿Ï…Í‚' or word == 'á¼‘Î±Ï…Ï„Î¿Ï…Í‚' else 0
			num_characters += len(word)

		return num_reflexive / num_characters

	def freq_vocative_sentences(file):
		file = TokenizeSentence("greek").tokenize_sentences(file)
		num_vocative = 0

		for line in file:
			num_vocative += 1 if 'á½¦' in line else 0

		return num_vocative / len(file)

	def freq_superlative(file):
		file = WordTokenizer('greek').tokenize(file)
		num_superlative = 0
		num_characters = 0

		for word in file:
			num_superlative += len(word) if word.endswith(('Ï„Î±Ï„Î¿Ï‚', 'Ï„Î¬Ï„Î¿Ï…', 'Ï„Î¬Ï„á¿³', 'Ï„Î±Ï„Î¿Î½', 'Ï„Î±Ï„Î¿Î¹', 'Ï„Î¬Ï„Ï‰Î½', \
				'Ï„Î¬Ï„Î¿Î¹Ï‚', 'Ï„Î¬Ï„Î¿Ï…Ï‚', 'Ï„Î¬Ï„Î·', 'Ï„Î¬Ï„Î·Ï‚', 'Ï„Î¬Ï„á¿ƒ', 'Ï„Î¬Ï„Î·Î½', 'Ï„Î±Ï„Î±Î¹', 'Ï„Î¬Ï„Î±Î¹Ï‚', 'Ï„Î¬Ï„Î±Ï‚', 'Ï„Î±Ï„Î±')) else 0
			num_characters += len(word)

		return num_superlative / num_characters

	def freq_conjunction(file):
		file = WordTokenizer('greek').tokenize(file)
		num_conjunction = 0
		num_characters = 0

		for word in file:
			num_conjunction += len(word) if word in {'Ï„Îµ', 'ÎºÎ±Î¯', 'Î´Î­', 'á¼€Î»Î»Î¬', 'ÎºÎ±Î¯Ï„Î¿Î¹', 'Î¿á½Î´Î­', 'Î¼Î·Î´Î­', 'á¼¤'} else 0
			num_characters += len(word)

		return num_conjunction / num_characters

	def mean_sentence_length(file):
		file = TokenizeSentence("greek").tokenize_sentences(file)
		lens = 0

		for line in file:
			lens += len(line)

		return lens / len(file)

	def non_interoggative_sentence_with_relative_clause(file):
		file = TokenizeSentence("greek").tokenize_sentences(file)
		num_sentence_with_clause = 0
		num_non_interrogative_sentence = 0
		interrogative_chars = {';', 'Í¾'} #Second character is Greek semi colon
		pronouns = {'á½…Ï‚', 'á½ƒÏ‚', 'Î¿á½—', 'á¾§', 'á½…Î½', 'á½ƒÎ½', 'Î¿á¼µ', 'Î¿á¼³', 'á½§Î½', 'Î¿á¼·Ï‚', 'Î¿á½•Ï‚', 'Î¿á½“Ï‚', 'á¼¥', 'á¼£', 'á¾—Ï‚', \
		'á¼¥Î½', 'á¼£Î½', 'Î±á¼µ', 'Î±á¼³', 'Î±á¼·Ï‚', 'á¼…Ï‚', 'á¼ƒÏ‚', 'á½…', 'á½ƒ', 'á¼…', 'á¼ƒ'}
		pronouns = pronouns | \
		{normalize('NFD', val) for val in pronouns} | \
		{normalize('NFC', val) for val in pronouns} | \
		{normalize('NFKD', val) for val in pronouns} | \
		{normalize('NFKC', val) for val in pronouns}

		for line in file:
			if line[-1] not in interrogative_chars:
				line = WordTokenizer('greek').tokenize(line)
				for word in line:
					if word in pronouns:
						num_sentence_with_clause += 1
						break
				num_non_interrogative_sentence += 1

		return num_sentence_with_clause / num_non_interrogative_sentence

	# Omit for now
	# def mean_length_relative_clause(file):
	# 	return 0

	#Count of relative pronouns in non-interrogative sentences / total non-interrogative sentences
	def relative_clause_per_sentence(file):
		file = TokenizeSentence("greek").tokenize_sentences(file)
		num_relative_pronoun = 0
		num_non_interrogative_sentence = 0
		pronouns = {'á½…Ï‚', 'á½ƒÏ‚', 'Î¿á½—', 'á¾§', 'á½…Î½', 'á½ƒÎ½', 'Î¿á¼µ', 'Î¿á¼³', 'á½§Î½', 'Î¿á¼·Ï‚', 'Î¿á½•Ï‚', 'Î¿á½“Ï‚', 'á¼¥', 'á¼£', 'á¾—Ï‚', \
		'á¼¥Î½', 'á¼£Î½', 'Î±á¼µ', 'Î±á¼³', 'Î±á¼·Ï‚', 'á¼…Ï‚', 'á¼ƒÏ‚', 'á½…', 'á½ƒ', 'á¼…', 'á¼ƒ'}
		pronouns = pronouns | \
		{normalize('NFD', val) for val in pronouns} | \
		{normalize('NFC', val) for val in pronouns} | \
		{normalize('NFKD', val) for val in pronouns} | \
		{normalize('NFKC', val) for val in pronouns}

		for line in file:
			if not line.endswith(';'): #TODO what if line ends in quote or bracket?
				for word in line.split():
					num_relative_pronoun += 1 if word in pronouns else 0
				num_non_interrogative_sentence += 1

		return num_relative_pronoun / num_non_interrogative_sentence

	#Count of all standalone instances of á¼”Ï€ÎµÎ¹Ï„Î±, á½…Î¼Ï‰Ï‚, ÎºÎ±Î¯Ï€ÎµÏ, á¼…Ï„Îµ, Î¿á¼·Î±
	def freq_circumstantial_participial_clauses(file):
		file = WordTokenizer('greek').tokenize(file)
		num_participles_characters = 0
		num_characters = 0
		participles = {'á¼”Ï€ÎµÎ¹Ï„Î±', 'á½…Î¼Ï‰Ï‚', 'ÎºÎ±Î¯Ï€ÎµÏ', 'á¼…Ï„Îµ', 'Î¿á¼·Î±'}

		for word in file:
			num_participles_characters += len(word) if word in participles else 0
			num_characters += len(word)

		return num_participles_characters / num_characters

	def freq_purpose_clause(file):
		file = WordTokenizer('greek').tokenize(file)
		num_purpose_characters = 0
		num_characters = 0
		purpose_characters = {'á¼µÎ½Î±', 'á½ƒÏ€Ï‰Ï‚'}

		for word in file:
			num_purpose_characters += len(word) if word in purpose_characters else 0
			num_characters += len(word)

		return num_purpose_characters / num_characters

	def freq_ws(file):
		file = WordTokenizer('greek').tokenize(file)
		num_ws_characters = 0
		num_characters = 0
		ws_characters = 'á½¡Ï‚'

		for word in file:
			num_ws_characters += len(word) if word == ws_characters else 0
			num_characters += len(word)

		return num_ws_characters / num_characters

	def ratio_ina_to_opos(file):
		file = WordTokenizer('greek').tokenize(file)
		num_ina = 0
		num_opos = 0

		for word in file:
			if word == 'á¼µÎ½Î±':
				num_ina += 1
			elif word == 'á½ƒÏ€Ï‰Ï‚':
				num_opos += 1

		return math.nan if num_ina == 0 and num_opos == 0 else math.inf if num_opos == 0 else num_ina / num_opos

	def freq_wste_not_precceded_by_eta(file):
		file = WordTokenizer('greek').tokenize(file)
		num_wste_characters = 0
		num_characters = 0
		wste_characters = 'á½¥ÏƒÏ„Îµ'
		ok_to_add = True

		for word in file:
			num_wste_characters += len(word) if word == wste_characters and ok_to_add else 0
			num_characters += len(word)
			ok_to_add = word != 'á¼¤'

		return num_wste_characters / num_characters

	def freq_wste_precceded_by_eta(file):
		file = WordTokenizer('greek').tokenize(file)
		num_wste_characters = 0
		num_characters = 0
		wste_characters = 'á½¥ÏƒÏ„Îµ'
		ok_to_add = False

		for word in file:
			num_wste_characters += len(word) if word == wste_characters and ok_to_add else 0
			num_characters += len(word)
			ok_to_add = word == 'á¼¤'

		return num_wste_characters / num_characters

	def freq_temporal_and_causal_clauses(file):
		file = WordTokenizer('greek').tokenize(file)
		num_clause_characters = 0
		num_characters = 0
		clause_chars = {'Î¼Î­Ï°ÏÎ¹', 'á¼•Ï‰Ï‚', 'Ï€ÏÎ¯Î½', 'Ï€Ïá½¶Î½', 'á¼Ï€ÎµÎ¯', 'á¼Ï€Îµá½¶', 'á¼Ï€ÎµÎ¹Î´Î®', 'á¼Ï€ÎµÎ¹Î´á½´', 'á¼Ï€ÎµÎ¹Î´Î¬Î½', 'á¼Ï€ÎµÎ¹Î´á½°Î½', 'á½…Ï„Îµ', 'á½…Ï„Î±Î½'}
		clause_chars = clause_chars | \
		{normalize('NFD', val) for val in clause_chars} | \
		{normalize('NFC', val) for val in clause_chars} | \
		{normalize('NFKD', val) for val in clause_chars} | \
		{normalize('NFKC', val) for val in clause_chars}

		for word in file:
			num_clause_characters += len(word) if word in clause_chars else 0
			num_characters += len(word)

		return num_clause_characters / num_characters

	def variance_of_sentence_length(file):
		file = TokenizeSentence("greek").tokenize_sentences(file)
		num_sentences = 0
		total_len = 0

		for line in file:
			num_sentences += 1
			total_len += len(line)
		mean = total_len / num_sentences
		squared_difference = 0
		for line in file:
			squared_difference += (len(line) - mean) ** 2

		return squared_difference / num_sentences

	def particles_per_sentence(file):
		file = WordTokenizer('greek').tokenize(file)
		num_particles = 0
		particles = {'á¼„Î½', 'á¼‚Î½', 'á¼†ÏÎ±', 'Î³Îµ', "Î³'", "Î´'", 'Î´Î­', 'Î´á½²','Î´Î®', 'Î´á½´', 'á¼•Ï‰Ï‚', "Îº'", 'ÎºÎµ', 'ÎºÎ­', 'Îºá½²', 'ÎºÎ­Î½', 'Îºá½²Î½', \
		'ÎºÎµÎ½', 'Î¼Î¬', 'Î¼á½°' 'Î¼Î­Î½', 'Î¼á½²Î½', 'Î¼Î­Î½Ï„Î¿Î¹', 'Î¼Î®', 'Î¼á½´', 'Î¼Î®Î½', 'Î¼á½´Î½', 'Î¼á¿¶Î½', 'Î½Ï', 'Î½á½º', 'Î½Ï…', 'Î¿á½', 'Î¿á½”', 'Î¿á½’', 'Î¿á½–Î½', \
		'Ï€ÎµÏ', 'Ï€Ï‰', "Ï„'", 'Ï„Îµ', 'Ï„Î¿Î¹'}
		particles = particles | \
		{normalize('NFD', val) for val in particles} | \
		{normalize('NFC', val) for val in particles} | \
		{normalize('NFKD', val) for val in particles} | \
		{normalize('NFKC', val) for val in particles}

		for word in file:
			num_particles += 1 if word in particles else 0

		num_sentences = file.count('.') + file.count(';') + file.count('Í¾') #Greek semi colon
		return num_particles / num_sentences

	def freq_raised_dot(file):
		#Unicode from https://en.wikipedia.org/wiki/Interpunct#Similar_symbols
		#'\u00B7' is 'Â·', '\u0387' is 'Î‡', '\u2219' is 'âˆ™', '\u22C5' is 'â‹…', '\u2022' is 'â€¢', '\u16EB' is 'á›«', '\u2027' is 'â€§', 
		#'\u2981' is 'â¦', '\u2E33' is 'â¸³', '\u30FB' is 'ãƒ»', '\uA78F' is 'êž', '\uFF65' is 'ï½¥', '\U00010101' is 'ð„'
		dot_chars = {'Â·', 'Î‡', 'âˆ™', 'â‹…', 'â€¢', 'á›«', 'â€§', 'â¦', 'â¸³', 'ãƒ»', 'êž', 'ï½¥', 'ð„'}
		num_dot_chars = 0
		for char in file:
			num_dot_chars += 1 if char in dot_chars else 0
		return num_dot_chars / len(file)

	def freq_men(file):
		file = WordTokenizer('greek').tokenize(file)
		men_chars = {'Î¼Î­Î½', 'Î¼á½²Î½'}
		men_chars = men_chars | \
		{normalize('NFD', val) for val in men_chars} | \
		{normalize('NFC', val) for val in men_chars} | \
		{normalize('NFKD', val) for val in men_chars} | \
		{normalize('NFKC', val) for val in men_chars}
		num_men = 0
		num_characters = 0
		for word in file:
			num_men += 1 if word in men_chars else 0
			num_characters += 1
		return num_men / num_characters


tesserae_clone_command = "git clone https://github.com/tesserae/tesserae.git"
greek_text_dir = "tesserae/texts/grc"

def main():
	global greek_text_dir

	#Associates files names to their respective features
	text_to_features = {}

	file_names = None
	if len(sys.argv) > 1 and sys.argv[1] == "testfile":
		file_names = {"tesserae/texts/grc/plato.respublica.tess"}

	#Download corpus if non-existent
	if not os.path.isdir(greek_text_dir):
		print("Corpus at " + greek_text_dir + " does not exist - attempting to clone repository...")
		os.system(tesserae_clone_command)

	#Obtain all the files to parse by traversing through the directory
	if file_names is None:
		file_names = {current_path + os.sep + current_file_name for current_path, current_dir_names, current_file_names in \
		os.walk(greek_text_dir) for current_file_name in current_file_names if current_file_name.endswith(".tess")}

	"""
	Certain files in the tesserae corpus are not well formatted so they will be ommitted

	This file is very ill-formed
	tesserae/texts/grc/bacchylides.dithyrambs.tess

	The following dictionaries are a mapping from a character to the frequency they appear at the end of a sentence 
	(tokenized by the cltk sentence parser). Since we only want periods and semi colons, we will remove files with 
	large occurrences of characters that are not periods or semi colons. For example, polybius.histories.tess has 
	66 double quotes, 0 single quotes, and 62 right brackets at the end of its tokenized sentences.
	Since it has a large number of quotes and brackets, we will omit it.

	tesserae/texts/grc/polybius.histories.tess
		{": 66, ': 0, ]: 62}
	tesserae/texts/grc/apollonius.argonautica.tess
		{": 143, ': 0, ]: 2}
	tesserae/texts/grc/apollonius.argonautica/apollonius.argonautica.part.1.tess
		{": 28, ': 0, ]: 1}
	tesserae/texts/grc/apollonius.argonautica/apollonius.argonautica.part.3.tess
		{": 49, ': 0, ]: 0}
	tesserae/texts/grc/apollonius.argonautica/apollonius.argonautica.part.2.tess
		{": 31, ': 0, ]: 1}
	tesserae/texts/grc/apollonius.argonautica/apollonius.argonautica.part.4.tess
		{": 35, ': 0, ]: 0}
	tesserae/texts/grc/athenaeus.deipnosophists.tess
		{": 0, ': 183, ]: 0}
	tesserae/texts/grc/theophrastus.characters.tess
		{": 1, ': 20, ]: 0}
	"""
	file_names -= {'tesserae/texts/grc/bacchylides.dithyrambs.tess', \
	'tesserae/texts/grc/polybius.histories.tess', \
	'tesserae/texts/grc/apollonius.argonautica.tess', \
	'tesserae/texts/grc/apollonius.argonautica/apollonius.argonautica.part.1.tess', \
	'tesserae/texts/grc/apollonius.argonautica/apollonius.argonautica.part.3.tess', \
	'tesserae/texts/grc/apollonius.argonautica/apollonius.argonautica.part.2.tess', \
	'tesserae/texts/grc/apollonius.argonautica/apollonius.argonautica.part.4.tess', \
	'tesserae/texts/grc/athenaeus.deipnosophists.tess', \
	'tesserae/texts/grc/theophrastus.characters.tess'}

	#Feature extraction
	for file_name in file_names:
		text_to_features[file_name] = {}

		#Store each line of file in a list
		file_text = []
		with open(file_name, mode="r", encoding="utf-8") as file:
			for line in file:
				#Ignore lines without tess tags, or parse the tag out and strip whitespace
				if not line.startswith("<"):
					continue
				assert ">" in line
				line = line[line.index(">") + 1:].strip()
				file_text.append(line)

		#Convert list of strings into a single string
		file_text = " ".join(file_text)

		#Invoke the values of the Feature class which are functions
		#Default behavior is to invoke ALL functions of Features class. If names of features are specified on the 
		#command line, then only invoke those
		for feature in Features.__dict__.values() if len(sys.argv) == 1 else \
		[Features.__dict__[sys.argv[i]] for i in range(1, len(sys.argv)) if sys.argv[i] in Features.__dict__]:
			if callable(feature):
				score = feature(file_text)
				text_to_features[file_name][feature] = score
				print(file_name + ", " + str(feature) + ", " + str(score))

if __name__ == "__main__":
	main()

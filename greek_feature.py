# -*- coding: utf-8 -*-

import os
import sys
import math
from cltk.tokenize.sentence import TokenizeSentence
from cltk.tokenize.word import WordTokenizer
from cltk.stem.lemma import LemmaReplacer
from unicodedata import normalize #the cltk_normalize cannot decompose (only has NFC & NFKC, not NFD or NFKD)
#Reference: https://jktauber.com/articles/python-unicode-ancient-greek/

class Features:
	def freq_interrogatives(file):
		file = TokenizeSentence("greek").tokenize_sentences(file)
		num_interrogative = 0

		for line in file:
			num_interrogative += line.count(";")

		return num_interrogative / len(file)

	def freq_conditional_characters(file):
		file = WordTokenizer('greek').tokenize(file)
		num_conditional_characters = 0
		num_characters = 0

		for word in file:
			num_conditional_characters += len(word) if word == "Îµá¼²" or word == "á¼Î¬Î½" or word == "ÎµÎ¹Ì“" else 0#Accent should be acute?
			num_characters += len(word)

		return num_conditional_characters / num_characters

	def freq_personal_pronouns(file):
		file = LemmaReplacer('greek').lemmatize(file)
		num_pronouns = 0
		num_characters = 0

		for word in file:
			num_pronouns += len(word) if word == "á¼Î³ÏŽ" or word == "á¼Î¼Î­Ï‰" or word == "ÏƒÏ" or word == "ÏƒÎµÏÏ‰" else 0
			num_characters += len(word)

		return num_pronouns / num_characters

	def freq_demonstrative(file):
		file = LemmaReplacer('greek').lemmatize(file)
		num_demonstratives = 0
		num_characters = 0

		for word in file:
			num_demonstratives += len(word) if word == 'á¼ÎºÎµÎ¹Í‚Î½Î¿Ï‚' or word == 'Î±á½•Ï„Î±Î¹' or word == 'Î¿á½—Ï„Î¿Ï‚' or word == 'Ï„ÏŒÎ´Îµ' \
			or word == 'Ï„Î¬Î´Îµ' or word == 'á¼ÎºÎµÎ¯Î½á¾±Ï‚' or word == 'Ï„Î¬ÏƒÏƒÏ‰' or word == 'Ï„Î±ÏÏ„á¾±Ï‚' or word == 'á½…Î´Îµ' else 0
			num_characters += len(word)

		return num_demonstratives / num_characters

	def freq_indefinite_pronoun_in_non_interrogative_sentence(file):
		# Extremely time consuming
		# file = TokenizeSentence("greek").tokenize_sentences(file)
		# num_pronouns = 0
		# num_characters = 0
		# for line in file:
			# line = LemmaReplacer('greek').lemmatize(line)
			# if ';' not in line:
			# 	for word in line:
			# 		num_pronouns += len(word) if word == 'Ï„Î¹Ï‚' else 0
			# 		num_characters += len(word)

		#TODO Need to check for interrogatives
		file = LemmaReplacer('greek').lemmatize(file)
		num_pronouns = 0
		num_characters = 0

		for word in file:
			num_pronouns += len(word) if word == 'Ï„Î¹Ï‚' else 0
			num_characters += len(word)

		return num_pronouns / num_characters

	def freq_allos(file):
		file = LemmaReplacer('greek').lemmatize(file)
		num_allos = 0
		num_characters = 0

		for word in file:
			num_allos += len(word) if word == 'á¼„Î»Î»Î¿Ï‚' or word == 'á¼„Î»Î»á¾±Ï‚' else 0
			num_characters += len(word)

		return num_allos / num_characters

	def freq_autos(file):
		file = LemmaReplacer('greek').lemmatize(file)
		num_autos = 0
		num_characters = 0

		for word in file:
			num_autos += len(word) if word == 'Î±Ï…Ì“Ï„ÏŒÏ‚' or word == 'Î±á½Ï„á¾±Ï‚' else 0
			num_characters += len(word)

		return num_autos / num_characters

	def freq_reflexive(file):
		file = LemmaReplacer('greek').lemmatize(file)
		num_reflexive = 0
		num_characters = 0

		for word in file:
			num_reflexive += len(word) if word == 'á¼Î¼Î±Ï…Ï„Î¿Ï…Í‚' or word == 'ÏƒÎ±Ï…Ï„Î¿Ï…Í‚' or word == 'á¼‘Î±Ï…Ï„Î¿Ï…Í‚' else 0
			num_characters += len(word)

		return num_reflexive / num_characters

	def freq_vocative_sentences(file):
		file = TokenizeSentence("greek").tokenize_sentences(file)
		num_vocative = 0

		for line in file:
			num_vocative += 1 if 'á½¦' in line else 0

		return num_vocative / len(file)

	def freq_superlative(file):
		file = WordTokenizer('greek').tokenize(file)
		num_superlative = 0
		num_characters = 0

		for word in file:
			num_superlative += len(word) if word.endswith(('Ï„Î±Ï„Î¿Ï‚', 'Ï„Î¬Ï„Î¿Ï…', 'Ï„Î¬Ï„á¿³', 'Ï„Î±Ï„Î¿Î½', 'Ï„Î±Ï„Î¿Î¹', 'Ï„Î¬Ï„Ï‰Î½', \
				'Ï„Î¬Ï„Î¿Î¹Ï‚', 'Ï„Î¬Ï„Î¿Ï…Ï‚', 'Ï„Î¬Ï„Î·', 'Ï„Î¬Ï„Î·Ï‚', 'Ï„Î¬Ï„á¿ƒ', 'Ï„Î¬Ï„Î·Î½', 'Ï„Î±Ï„Î±Î¹', 'Ï„Î¬Ï„Î±Î¹Ï‚', 'Ï„Î¬Ï„Î±Ï‚', 'Ï„Î±Ï„Î±')) else 0
			num_characters += len(word)

		return num_superlative / num_characters

	def freq_conjunction(file):
		file = WordTokenizer('greek').tokenize(file)
		num_conjunction = 0
		num_characters = 0

		for word in file:
			num_conjunction += len(word) if word in {'Ï„Îµ', 'ÎºÎ±Î¯', 'Î´Î­', 'á¼€Î»Î»Î¬', 'ÎºÎ±Î¯Ï„Î¿Î¹', 'Î¿á½Î´Î­', 'Î¼Î·Î´Î­', 'á¼¤'} else 0
			num_characters += len(word)

		return num_conjunction / num_characters

	def mean_sentence_length(file):
		file = TokenizeSentence("greek").tokenize_sentences(file)
		lens = 0

		for line in file:
			lens += len(line)

		return lens / len(file)

	def non_interoggative_sentence_with_relative_clause(file):
		return 0

	def mean_length_relative_clause(file):
		return 0

	#Count of relative pronouns in non-interrogative sentences / total non-interrogative sentences
	def relative_clause_per_sentence(file):
		file = TokenizeSentence("greek").tokenize_sentences(file)
		num_relative_pronoun = 0
		num_non_interrogative_sentence = 0
		pronouns = {'á½…Ï‚', 'Î¿á½—', 'á¾§', 'á½…Î½', 'Î¿á¼µ', 'á½§Î½', 'Î¿á¼·Ï‚', 'Î¿á½•Ï‚', 'á¼¥', 'á¾—Ï‚', 'á¼¥Î½', 'Î±á¼µ', 'Î±á¼·Ï‚', 'á¼…Ï‚', 'á½…', 'á¼…'}

		for line in file:
			if not line.endswith(';'): #TODO what if line ends in quote or bracket?
				for word in line.split():
					num_relative_pronoun += 1 if word in pronouns else 0
				num_non_interrogative_sentence += 1

		return num_relative_pronoun / num_non_interrogative_sentence

	#Count of all standalone instances of á¼”Ï€ÎµÎ¹Ï„Î±, á½…Î¼Ï‰Ï‚, ÎºÎ±Î¯Ï€ÎµÏ, á¼…Ï„Îµ, Î¿á¼·Î±
	def freq_circumstantial_participial_clauses(file):
		file = WordTokenizer('greek').tokenize(file)
		num_participles_characters = 0
		num_characters = 0
		participles = {'á¼”Ï€ÎµÎ¹Ï„Î±', 'á½…Î¼Ï‰Ï‚', 'ÎºÎ±Î¯Ï€ÎµÏ', 'á¼…Ï„Îµ', 'Î¿á¼·Î±'}

		for word in file:
			num_participles_characters += len(word) if word in participles else 0
			num_characters += len(word)

		return num_participles_characters / num_characters

	def freq_purpose_clause(file):
		file = WordTokenizer('greek').tokenize(file)
		num_purpose_characters = 0
		num_characters = 0
		purpose_characters = {'á¼µÎ½Î±', 'á½ƒÏ€Ï‰Ï‚'}

		for word in file:
			num_purpose_characters += len(word) if word in purpose_characters else 0
			num_characters += len(word)

		return num_purpose_characters / num_characters

	def freq_ws(file):
		file = WordTokenizer('greek').tokenize(file)
		num_ws_characters = 0
		num_characters = 0
		ws_characters = 'á½¡Ï‚'

		for word in file:
			num_ws_characters += len(word) if word == ws_characters else 0
			num_characters += len(word)

		return num_ws_characters / num_characters

	def ratio_ina_to_opos(file):
		file = WordTokenizer('greek').tokenize(file)
		num_ina = 0
		num_opos = 0

		for word in file:
			if word == 'á¼µÎ½Î±':
				num_ina += 1
			elif word == 'á½ƒÏ€Ï‰Ï‚':
				num_opos += 1

		return math.nan if num_ina == 0 and num_opos == 0 else math.inf if num_opos == 0 else num_ina / num_opos

	def freq_wste_not_precceded_by_eta(file):
		file = WordTokenizer('greek').tokenize(file)
		num_wste_characters = 0
		num_characters = 0
		wste_characters = 'á½¥ÏƒÏ„Îµ'
		ok_to_add = True

		for word in file:
			num_wste_characters += len(word) if word == wste_characters and ok_to_add else 0
			num_characters += len(word)
			ok_to_add = word != 'á¼¤'

		return num_wste_characters / num_characters

	def freq_wste_precceded_by_eta(file):
		file = WordTokenizer('greek').tokenize(file)
		num_wste_characters = 0
		num_characters = 0
		wste_characters = 'á½¥ÏƒÏ„Îµ'
		ok_to_add = False

		for word in file:
			num_wste_characters += len(word) if word == wste_characters and ok_to_add else 0
			num_characters += len(word)
			ok_to_add = word == 'á¼¤'

		return num_wste_characters / num_characters

	def freq_temporal_and_causal_clauses(file):
		file = WordTokenizer('greek').tokenize(file)
		num_clause_characters = 0
		num_characters = 0
		clause_chars = {'Î¼Î­Ï°ÏÎ¹', 'á¼•Ï‰Ï‚', 'Ï€ÏÎ¯Î½', 'Ï€Ïá½¶Î½', 'á¼Ï€ÎµÎ¯', 'á¼Ï€Îµá½¶', 'á¼Ï€ÎµÎ¹Î´Î®', 'á¼Ï€ÎµÎ¹Î´á½´', 'á¼Ï€ÎµÎ¹Î´Î¬Î½', 'á¼Ï€ÎµÎ¹Î´á½°Î½', 'á½…Ï„Îµ', 'á½…Ï„Î±Î½'}
		clause_chars = clause_chars | \
		{normalize('NFD', val) for val in clause_chars} | \
		{normalize('NFC', val) for val in clause_chars} | \
		{normalize('NFKD', val) for val in clause_chars} | \
		{normalize('NFKC', val) for val in clause_chars}

		for word in file:
			num_clause_characters += len(word) if word in clause_chars else 0
			num_characters += len(word)

		return num_clause_characters / num_characters

	def variance_of_sentence_length(file):
		file = TokenizeSentence("greek").tokenize_sentences(file)
		num_sentences = 0
		total_len = 0

		for line in file:
			num_sentences += 1
			total_len += len(line)
		mean = total_len / num_sentences
		squared_difference = 0
		for line in file:
			squared_difference += (len(line) - mean) ** 2

		return squared_difference / num_sentences

	def particles_per_sentence(file):
		file = WordTokenizer('greek').tokenize(file)
		num_particles = 0
		particles = {'á¼„Î½', 'á¼‚Î½', 'á¼†ÏÎ±', 'Î³Îµ', "Î³'", "Î´'", 'Î´Î­', 'Î´á½²','Î´Î®', 'Î´á½´', 'á¼•Ï‰Ï‚', "Îº'", 'ÎºÎµ', 'ÎºÎ­', 'Îºá½²', 'ÎºÎ­Î½', 'Îºá½²Î½', \
		'ÎºÎµÎ½', 'Î¼Î¬', 'Î¼á½°' 'Î¼Î­Î½', 'Î¼á½²Î½', 'Î¼Î­Î½Ï„Î¿Î¹', 'Î¼Î®', 'Î¼á½´', 'Î¼Î®Î½', 'Î¼á½´Î½', 'Î¼á¿¶Î½', 'Î½Ï', 'Î½á½º', 'Î½Ï…', 'Î¿á½', 'Î¿á½”', 'Î¿á½’', 'Î¿á½–Î½', \
		'Ï€ÎµÏ', 'Ï€Ï‰', "Ï„'", 'Ï„Îµ', 'Ï„Î¿Î¹'}
		particles = particles | \
		{normalize('NFD', val) for val in particles} | \
		{normalize('NFC', val) for val in particles} | \
		{normalize('NFKD', val) for val in particles} | \
		{normalize('NFKC', val) for val in particles}

		for word in file:
			num_particles += 1 if word in particles else 0

		num_sentences = file.count('.') + file.count(';') + file.count('Í¾') #Greek semi colon
		return num_particles / num_sentences

	def freq_raised_dot(file):
		#Unicode from https://en.wikipedia.org/wiki/Interpunct#Similar_symbols
		#'\u00B7' is 'Â·', '\u0387' is 'Î‡', '\u2219' is 'âˆ™', '\u22C5' is 'â‹…', '\u2022' is 'â€¢', '\u16EB' is 'á›«', '\u2027' is 'â€§', '\u2981' is 'â¦', '\u2E33' is 'â¸³', '\u30FB' is 'ãƒ»', '\uA78F' is 'êž', '\uFF65' is 'ï½¥', '\U00010101' is 'ð„'
		dot_chars = {'Â·', 'Î‡', 'âˆ™', 'â‹…', 'â€¢', 'á›«', 'â€§', 'â¦', 'â¸³', 'ãƒ»', 'êž', 'ï½¥', 'ð„'}
		num_dot_chars = 0
		for char in file:
			num_dot_chars += 1 if char in dot_chars else 0
		return num_dot_chars / len(file)

	def freq_men(file):
		file = WordTokenizer('greek').tokenize(file)
		men_chars = {'Î¼Î­Î½', 'Î¼á½²Î½'}
		men_chars = men_chars | \
		{normalize('NFD', val) for val in men_chars} | \
		{normalize('NFC', val) for val in men_chars} | \
		{normalize('NFKD', val) for val in men_chars} | \
		{normalize('NFKC', val) for val in men_chars}
		num_men = 0
		num_characters = 0
		for word in file:
			num_men += 1 if word in men_chars else 0
			num_characters += 1
		return num_men / num_characters


tesserae_clone_command = "git clone https://github.com/tesserae/tesserae.git"
greek_text_dir = "tesserae/texts/grc"

def main():
	global greek_text_dir

	#Associates files names to their respective features
	text_to_features = {}

	file_names = None
	if len(sys.argv) > 1:
		if sys.argv[1] == "debug": #if debug, just scan pre-selected corpus
			file_names = ["tesserae/texts/grc/polybius.histories.tess"]
		else: #Allows user to select custom path other than tesserae
			greek_text_dir = sys.argv[1]

	#Download corpus if non-existent
	if not os.path.isdir(greek_text_dir):
		print("Corpus at " + greek_text_dir + " does not exist - attempting to clone repository...")
		os.system(tesserae_clone_command)

	#Obtain all the files to parse by traversing through the directory
	if file_names is None:
		file_names = [current_path + os.sep + current_file_name for current_path, current_dir_names, current_file_names in \
		os.walk(greek_text_dir) for current_file_name in current_file_names if current_file_name.endswith(".tess")]

	#Feature extraction
	for file_name in file_names:
		text_to_features[file_name] = {}

		#Store each line of file in a list
		file_text = []
		with open(file_name, mode="r", encoding="utf-8") as file:
			for line in file:
				#Ignore lines without tess tags, or parse the tag out and strip whitespace
				if not line.startswith("<"):
					continue
				assert ">" in line
				line = line[line.index(">") + 1:].strip()
				file_text.append(line)

		#Convert list of strings into a single string
		file_text = " ".join(file_text)

		#Invoke the values of the Feature class which are functions
		for feature in Features.__dict__.values():
			if callable(feature):
				score = feature(file_text)
				text_to_features[file_name][feature] = score
				print(file_name + ", " + str(feature) + ", " + str(score))

if __name__ == "__main__":
	main()
